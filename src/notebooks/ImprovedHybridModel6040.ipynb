{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b670663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79a1f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5999 images belonging to 4 classes.\n",
      "Found 2576 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Enhanced Data Preparation\n",
    "train_dir = \"../../data5/train\"\n",
    "validation_dir = \"../../data5/test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2, \n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd23601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_improved_hybrid_model(num_classes, input_shape=(224, 224, 3)):\n",
    "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    for base_model in [resnet_base, vgg_base, inception_base]:\n",
    "        base_model.trainable = False\n",
    "\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    resnet_output = GlobalAveragePooling2D()(resnet_base(input_tensor))\n",
    "    vgg_output = GlobalAveragePooling2D()(vgg_base(input_tensor))\n",
    "    inception_output = GlobalAveragePooling2D()(inception_base(input_tensor))\n",
    "\n",
    "    # Add normalization before concatenation\n",
    "    resnet_output = BatchNormalization()(resnet_output)\n",
    "    vgg_output = BatchNormalization()(vgg_output)\n",
    "    inception_output = BatchNormalization()(inception_output)\n",
    "\n",
    "    merged = concatenate([resnet_output, vgg_output, inception_output])\n",
    "    x = Dense(512, activation='relu')(merged)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)  # Additional trainable layer\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    return model\n",
    "\n",
    "model = build_improved_hybrid_model(num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563b9dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liraj/.pyenv/versions/3.9.4/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1510s\u001b[0m 8s/step - AUC: 0.8171 - accuracy: 0.5769 - loss: 1.1652 - val_AUC: 0.8985 - val_accuracy: 0.6883 - val_loss: 0.8060 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1283s\u001b[0m 7s/step - AUC: 0.8966 - accuracy: 0.6816 - loss: 0.7819 - val_AUC: 0.9094 - val_accuracy: 0.6976 - val_loss: 0.7484 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 7s/step - AUC: 0.9072 - accuracy: 0.6976 - loss: 0.7381 - val_AUC: 0.9166 - val_accuracy: 0.7038 - val_loss: 0.6971 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1305s\u001b[0m 7s/step - AUC: 0.9153 - accuracy: 0.7130 - loss: 0.7011 - val_AUC: 0.9254 - val_accuracy: 0.7372 - val_loss: 0.6770 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 7s/step - AUC: 0.9179 - accuracy: 0.7110 - loss: 0.6891 - val_AUC: 0.9321 - val_accuracy: 0.7415 - val_loss: 0.6344 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 7s/step - AUC: 0.9213 - accuracy: 0.7230 - loss: 0.6767 - val_AUC: 0.9302 - val_accuracy: 0.7368 - val_loss: 0.6400 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1300s\u001b[0m 7s/step - AUC: 0.9294 - accuracy: 0.7392 - loss: 0.6383 - val_AUC: 0.9287 - val_accuracy: 0.7387 - val_loss: 0.6474 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 7s/step - AUC: 0.9280 - accuracy: 0.7363 - loss: 0.6468 - val_AUC: 0.9332 - val_accuracy: 0.7450 - val_loss: 0.6341 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1290s\u001b[0m 7s/step - AUC: 0.9299 - accuracy: 0.7325 - loss: 0.6351 - val_AUC: 0.9357 - val_accuracy: 0.7504 - val_loss: 0.6200 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1299s\u001b[0m 7s/step - AUC: 0.9337 - accuracy: 0.7452 - loss: 0.6192 - val_AUC: 0.9361 - val_accuracy: 0.7550 - val_loss: 0.6183 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "def train_model(model, train_generator, validation_generator, learning_rate=1e-3, epochs=10):\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    return history\n",
    "\n",
    " # Build and train model\n",
    "model = build_improved_hybrid_model(num_classes)\n",
    "train_history = train_model(model, train_generator, validation_generator, learning_rate=1e-3, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3473bbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 5s/step\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1. Enfeksiyonel       0.24      0.27      0.25       650\n",
      "      2. Ekzama       0.26      0.30      0.28       650\n",
      "        3. Akne       0.24      0.21      0.22       626\n",
      "      4. Malign       0.28      0.24      0.26       650\n",
      "\n",
      "       accuracy                           0.25      2576\n",
      "      macro avg       0.25      0.25      0.25      2576\n",
      "   weighted avg       0.25      0.25      0.25      2576\n",
      "\n",
      "[[175 186 147 142]\n",
      " [188 194 146 122]\n",
      " [186 175 132 133]\n",
      " [180 183 134 153]]\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m840s\u001b[0m 4s/step\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "1. Enfeksiyonel       0.24      0.23      0.23      1500\n",
      "      2. Ekzama       0.24      0.25      0.24      1500\n",
      "        3. Akne       0.26      0.28      0.27      1499\n",
      "      4. Malign       0.27      0.25      0.26      1500\n",
      "\n",
      "       accuracy                           0.25      5999\n",
      "      macro avg       0.25      0.25      0.25      5999\n",
      "   weighted avg       0.25      0.25      0.25      5999\n",
      "\n",
      "[[342 442 385 331]\n",
      " [371 371 422 336]\n",
      " [346 400 414 339]\n",
      " [371 363 386 380]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "val_preds = np.argmax(model.predict(validation_generator), axis=1)\n",
    "true_labels = validation_generator.classes\n",
    "print(classification_report(true_labels, val_preds, target_names=validation_generator.class_indices.keys()))\n",
    "print(confusion_matrix(true_labels, val_preds))\n",
    "# Classification report and confusion matrix\n",
    "val_preds = np.argmax(model.predict(train_generator), axis=1)\n",
    "true_labels = train_generator.classes\n",
    "print(classification_report(true_labels, val_preds, target_names=train_generator.class_indices.keys()))\n",
    "print(confusion_matrix(true_labels, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(history, metrics=[\"accuracy\", \"loss\"]):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics for each metric in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - history: The history object returned by model.fit().\n",
    "    - metrics: A list of metric names to plot (e.g., [\"accuracy\", \"loss\"]).\n",
    "    \"\"\"\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[metric], label=f\"Training {metric}\")\n",
    "        plt.plot(history.history[f\"val_{metric}\"], label=f\"Validation {metric}\")\n",
    "        plt.title(f\"Training vs Validation {metric.capitalize()}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Plot accuracy and loss for the initial training\n",
    "plot_training_results(train_history, metrics=[\"accuracy\", \"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fine-Tuning the Model\n",
    "def fine_tune_model(model, train_generator, validation_generator, base_models, fine_tune_layers, learning_rate=1e-5, epochs=10):\n",
    "    for base_model, layers_to_unfreeze in zip(base_models, fine_tune_layers):\n",
    "        for layer in base_model.layers[-layers_to_unfreeze:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    return fine_tune_history\n",
    "\n",
    "resnet_base = model.get_layer(\"resnet50\")\n",
    "vgg_base = model.get_layer(\"vgg16\")\n",
    "inception_base = model.get_layer(\"inception_v3\")\n",
    "fine_tune_layers = [20, 10, 15]  # Increased unfreezing\n",
    "\n",
    "fine_tune_history = fine_tune_model(\n",
    "    model,\n",
    "    train_generator,\n",
    "    validation_generator,\n",
    "    base_models=[resnet_base, vgg_base, inception_base],\n",
    "    fine_tune_layers=fine_tune_layers,\n",
    "    learning_rate=1e-5,\n",
    "    epochs=20\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(history, metrics=[\"accuracy\", \"loss\"]):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics for each metric in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - history: The history object returned by model.fit().\n",
    "    - metrics: A list of metric names to plot (e.g., [\"accuracy\", \"loss\"]).\n",
    "    \"\"\"\n",
    "    for metric in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[metric], label=f\"Training {metric}\")\n",
    "        plt.plot(history.history[f\"val_{metric}\"], label=f\"Validation {metric}\")\n",
    "        plt.title(f\"Training vs Validation {metric.capitalize()}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Plot accuracy and loss for the initial training\n",
    "plot_training_results(fine_tune_history, metrics=[\"accuracy\", \"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc2c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../../models/improved_hybrid_model4060.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the Improved Model\n",
    "results = model.evaluate(validation_generator)\n",
    "print(f\"Validation Loss: {results[0]:.2f}\")\n",
    "print(f\"Validation Accuracy: {results[1]:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Improved Feature Extraction for KNN and RF\n",
    "# feature_extractor = Model(inputs=model.input, outputs=model.layers[-3].output)\n",
    "feature_extractor = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "def extract_features(model, generator):\n",
    "    features = model.predict(generator, verbose=1)\n",
    "    labels = generator.classes\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(feature_extractor, train_generator)\n",
    "val_features, val_labels = extract_features(feature_extractor, validation_generator)\n",
    "\n",
    "pca = PCA(n_components=128)  # Dimensionality reduction\n",
    "train_features_pca = pca.fit_transform(train_features)\n",
    "val_features_pca = pca.transform(val_features)\n",
    "\n",
    "pca_model_path = \"../../models/pca_model4060.pkl\"\n",
    "joblib.dump(pca, pca_model_path)\n",
    "\n",
    "print(f\"PCA model saved at {pca_model_path}\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_features_pca, train_labels)\n",
    "knn_accuracy = accuracy_score(val_labels, knn.predict(val_features_pca))\n",
    "print(f\"KNN Validation Accuracy: {knn_accuracy:.2f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(train_features_pca, train_labels)\n",
    "rf_accuracy = accuracy_score(val_labels, rf.predict(val_features_pca))\n",
    "print(f\"Random Forest Validation Accuracy: {rf_accuracy:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the KNN model\n",
    "joblib.dump(knn, '../../models/improved_knn_model4060.pkl')\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(rf, '../../models/improved_random_forest_model6040.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plotting ROC Curve\n",
    "def plot_roc_curve(true_labels, predicted_probabilities, class_names):\n",
    "    if len(true_labels.shape) == 1:\n",
    "        true_labels = label_binarize(true_labels, classes=range(len(class_names)))\n",
    "\n",
    "    n_classes = len(class_names)\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], predicted_probabilities[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_labels.ravel(), predicted_probabilities.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"Class {class_names[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], linestyle='--', label=f\"Micro-average (AUC = {roc_auc['micro']:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Get Validation Results\n",
    "def get_validation_results(model, validation_generator):\n",
    "    validation_steps = validation_generator.samples // validation_generator.batch_size\n",
    "    true_labels = []\n",
    "    predicted_probabilities = []\n",
    "\n",
    "    for i in range(validation_steps):\n",
    "        x_batch, y_batch = next(validation_generator)\n",
    "        true_labels.extend(np.argmax(y_batch, axis=1))\n",
    "        predicted_probabilities.extend(model.predict(x_batch))\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_probabilities = np.array(predicted_probabilities)\n",
    "    return true_labels, predicted_probabilities\n",
    "\n",
    "# Evaluate and plot ROC Curve\n",
    "class_names = list(validation_generator.class_indices.keys())\n",
    "true_labels, predicted_probabilities = get_validation_results(model, validation_generator)\n",
    "plot_roc_curve(true_labels, predicted_probabilities, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
